{
  "query": "What is reinforcement learning?",
  "results": [
    "CoRR, abs/2411.15124, 2024. [27] Shilei Li, Meng Li, Jiongming Su, Shaofei Chen, Zhimin Yuan, and Qing Ye. PP-PG: combining parameter perturbation with policy gradient methods for effective and efficient explorations in deep reinforcement learning. ACM Trans. Intell. Syst. Technol., 12(3):35:1–35:21, 2021. [28] Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. CoRR, abs/2505.24864, 2025. [29] Xiaoyuan Liu, Tian Liang, Zhiwei He, Jiahao Xu, Wenxuan Wang, Pinjia He, Zhaopeng Tu, Haitao Mi, and Dong Yu. Trust, but verify: A self-verification approach to reinforcement learning with verifiable rewards. CoRR, abs/2505.13445, 2025. [30] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: A critical perspective. CoRR, abs/2503.20783, 2025. [31] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. CoRR, abs/2504.02495, 2025. [32] Tongxu Luo, Wenyu Du, Jiaxi Bi, Stephen Chung, Zhengyang Tang, Hao Yang, Min Zhang, and Benyou Wang. Learning from peers in reasoning models. CoRR, abs/2505.07787, 2025. [33] Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan Cao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua Lin, and Kai Chen. Exploring the limit of outcome reward for learning mathematical reasoning. CoRR, abs/2502.06781, 2025. [34] Yurii E. Nesterov and Vladimir G. Spokoiny. Random gradient-free minimization of convex functions. Found. Comput. Math., 17(2):527–566, 2017. [35] Kusha Sareen, Morgane M. Moss, Alessandro Sordoni, Rishabh Agarwal, and Arian Hosseini. Putting the value back in RL: better test-time scaling by unifying LLM reasoners with verifiers. CoRR, abs/2505.04842, 2025. [36] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. [37] Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, and Aviral Kumar. e3: Learning to explore enables extrapolation of test-time compute for llms. CoRR, abs/2506.09026, 2025. [38] Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer. Spurious rewards: Rethinking training signals in RLVR. CoRR, abs/2506.10947, 2025. [39] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. [40] Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron T. Parisi, Abhishek Kumar, Alexander A. Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, 22 Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. Beyond",
    "Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models Zhipeng Chen1,2,∗, Xiaobo Qin2, Youbin Wu2, Yue Ling2, Qinghao Ye2, Wayne Xin Zhao1,2,†, Guang Shi2,† 1Renmin University of China, 2ByteDance Seed ∗Work done at ByteDance Seed, †Corresponding authors Abstract Reinforcement learning with verifiable rewards (RLVR), which typically adopts Pass@1 as the reward, has faced the issues in balancing exploration and exploitation, causing policies to prefer conservative actions, converging to a local optimum. Identifying an appropriate reward metric is therefore crucial. Regarding the prior work, although Pass@k has been used in evaluation, its connection to LLM exploration ability in RLVR remains largely overlooked. To investigate this, we first use Pass@k as the reward to train the policy model (i.e., Pass@k Training), and observe the improvement on its exploration ability. Next, we derive an analytical solution for the advantage of Pass@k Training, leading to an efficient and effective process. Building on this, our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. Moreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we preliminarily explore the advantage design for RLVR, showing promising results and highlighting a potential future direction. Date: August 15, 2025 Correspondence: Zhipeng Chen at zhipeng_chen@ruc.edu.cn Project Page: https://github.com/RUCAIBox/Passk_Training Pass@1 Training Pass@K Training Pass@1 Training Vanilla RLVR (Pass@1 Acc.) Vanilla RLVR (Pass@K Acc.) GPT 4o (Pass@1 Acc.) Claude 3.7 (Pass@1 Acc.) Figure 1 Enigmata scores (Validation Set) of Pass@k Training on Qwen2.5-7B-Ins, which boosts its exploration ability, leading to continuous improvements in following training, surpassing native RLVR and powerful LLMs. 1 arXiv:2508.10751v1 [cs.LG] 14 Aug 2025 1 Introduction Recently, reinforcement learning with verifiable rewards (RLVR) has emerged to solve complex reasoning tasks and dramatically boost the reasoning capabilities of large language models (LLMs) [26, 44, 58]. During RLVR training, LLMs generate various responses based on the given prompt and receive rewards for responses [17, 39, 43]. LLMs can possess the ability to generate a more comprehensive reasoning process by learning from outcome-level supervision [9, 13], thereby achieving higher performance on downstream tasks. The success of these large reasoning models (LRMs), such as OpenAI o1 [23] and DeepSeek R1 [13], suggests that RLVR training pushes the limits of the capacities of LLMs. The current RLVR training that typically optimizes the Pass@1 objective, also known as Pass@1 Training, trains LLMs to learn from their exploration and generate the most confident response for the given prompt [21, 50, 54], leading to a major challenge of the balance of exploration and exploitation [10, 20]. Typically, exploration refers to performing novel and various actions [46], while exploitation requires LLMs to invoke reliable actions that the verifier prefers among the known behaviours [40]. During the Pass@1 Training process, LLMs tend to imitate the behaviours that can bring an increase of reward scores in previous attempts, and prevent the behaviours that receive low rewards [8, 33]. However, in outcome-supervision, which is the popular Pass@1 Training setting [13, 50, 54], the erroneous solution with correct answer will receive positive rewards, while the correct",
    "achieving less that 50% success rate. These shortcomings limit the practicality and scalability of LLMs, highlighting the need for new training methods to enhance the capabilities of more efficient mod- els. However, existing LLM post-training methods, those that refine and adapt a pre-trained model to meet application- specific requirements, are unsuitable for this domain. These approaches, often based on Reinforcement Learning (RL) (Sutton and Barto 2018), are designed to optimize models on single-turn tasks with immediate feedback from a verifier, as in Reinforcement Learning with Verifiable Rewards (RLVR) (DeepSeek-AI et al. 2025; Zheng et al. 2025a; Yu et al. 2025; Wang et al. 2025a; Hou et al. 2025; Park et al. 2025), or from human preference models, as in Reinforcement Learn- ing from Human Feedback (RLHF) (Ziegler et al. 2020; Ouyang et al. 2022; Rafailov et al. 2023; Zhong et al. 2025). Such methods, however, are incompatible with sequential decision-making tasks where credit assignment of outcomes to actions is necessary. Addressing this is an emerging re- search area. For example, the RAGEN system (Wang et al. 2025b) conditions the agent’s language generation on full environment episodes, assigning credit for the entire episode to the agent’s complete sequence of actions. Furthermore, a conceptual limitation arises when using LLMs as decision-making agents: the optimization occurs over sequences of tokens, which are communicative units rooted in natural language, whereas effective planning re- quires the selection of actions grounded in the problem do- main (e.g. navigation moves in a spatial environment). This discrepancy mirrors the distinction between communicative acts, such as speech acts in dialogue systems (Traum 1999), and operational actions needed for sequential decision- making (Georgeff 1988). Bridging this gap calls for new arXiv:2508.10839v1 [cs.CL] 14 Aug 2025 methods that formally align the language-centric outputs of LLMs with the structured, domain-specific actions required for agent planning and control. Against this background, for the first time, we: 1. Define a formal framework connecting language- based agents and sequential decision-making environ- ments, comprising the Text-Mediated Stochastic Game (TMSG), which models the environment with an explicit text interface, and Language Agent Policy (LAP), which defines the agent’s LLM-based policy. 2. Introduce Multi-Step Group-Relative Policy Optimiza- tion (MS-GRPO), an algorithm adapting the GRPO method for sequential decision-making tasks by assign- ing the entire cumulative episode reward to each individ- ual step. To improve efficiency, the optimization for each step uses only the current state as context. 3. Propose a novel absolute-advantage-weighted (AAW) episode sampling strategy which we demonstrate im- proves training performance. 4. Demonstrate that our post-trained 3B parameter model outperforms a much larger 72B parameter baseline LLM on the Frozen Lake task by 50%, showing the value of domain-specific training over model scale. 5. Provide a critical analysis of the MS-GRPO algorithm’s capabilities, highlighting its high training variance and mixed results in eliciting generalization in LLM-based agents. Framework This section defines our framework for language model- based agents in sequential decision-making environments. The framework consists of two core contributions: (1) a Text-Mediated Stochastic Game (TMSG), that formalizes an environment where all interactions are mediated exclu- sively through text;"
  ]
}