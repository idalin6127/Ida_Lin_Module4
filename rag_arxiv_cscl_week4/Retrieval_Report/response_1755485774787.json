{
  "query": "How do LLMs use FAISS?",
  "results": [
    "applied to tasks that existed prior to LLMs being introduced. The motivation for applying LLMs to these older tasks may be the desire to improve efficiency in execution speed, improve accessibility to non-technical users, or im- prove the state-of-the-art in accuracy. This broad spectrum of LLM applications is what has moti- vated us to evaluate IBEaM by generating numeric scores and using them as a proxy for performing classification and ranking tasks. One method of improving LLM performance on tasks is to perform chain of thought reasoning, where the LLM is walked through the process of solving one or more examples in the prompt prior to being asked to evaluate an instance (Wei et al., 2022). Another method of improving LLM per- formance is to perform prompt engineering (Chen et al., 2025), which is typically a repetitive process that optimizes an LLM’s downstream performance by making small adjustments to its prompt. All prompt engineering techniques fundamentally are attempting to make the prompt work better with the parameters learned by the LLM, or in other words, are attempting to find the prompt that is most compatible with the LLM’s inductive biases. All of the datasets examined in this paper have some concept of a world state implied by the text. LLMs are able to reason about the hypothetical world state implied in a prompt, and also the real world state implied by their training data (Zhu et al., 2023). The encoding of this real world state is a form of inductive bias in LLMs. The category of task that we examine in this pa- per is classification. Gretz et al. (2023) selected several classification tasks and LLMs and com- pared the LLMs’ zero-shot performance to their performance when fine tuned. Sun et al. (2023) also created a novel prompting technique to im- prove LLMs’ ability to perform text classification. Using LLMs to produce Likert ratings has been evaluated by Chiang and Lee (2023). In their work, they found that asking an LLM to explain the rating that it assigned improves performance over meth- ods that constrain the LLM output to a single rating integer. We apply this technique of explanation generation to IBEaM to optimize our performance. The concept of breaking a problem into more manageable sub-problems has also been researched previously. LLM-Eval is a system for improving LLM output by breaking a problem down into sub- tasks and prompting an LLM to fill a schema for the response (Lin and Chen, 2023). There are some in- teresting technical differences between LLM-Eval and our work. First, LLM-Eval uses schema filling, but we found that, in our particular prompt formu- 2 Figure 2: For each of our trials, we generate a Likert scale for each metric and apply it to all instances in that trial. We include the scale as part of the conversation history when making future calls to the LLM. Keeping the scale the same for all instances improves rating consistency and reduces the cost and computation time when compared to regenerating scales for each instance. lation for IBEaM, schema filling was",
    "year of these LLMs (referred to as new corpus). For each domain, we divide idioms into five groups according to the 1st, 2nd and 3rd quartiles of the overall frequency: VH (very high), H (high), M (medium), L (low), and N (never appear in the data). This allows us to measure the impact of idioms frequency in pre- training corpora. We use the old corpus (data before 2023 in the same domains, details in Appendix B.1) to calculate idiom frequencies, as idioms from this period are more likely to be included in the LLMs’ training data. Given that idioms tend to have a long-standing presence in a language, their frequency distribution within the same domain is unlikely to change significantly over time. Table 2 displays statistics of the collected data. The details of data collection for each domain are described in Appendix C.1. 3.2 Collect Translations from Systems We carefully select a diverse set of translation systems to allow us to assess idiom translation performance across a range of system types: GPT-4 (a SOTA LLM) (Achiam et al., 2023), Alma-13B (a fine-tuned translation model) (Xu et al., 2024), Qwen-14B-Chat (Bai et al., 2023) (a bilingual LLM for Chinese and English), mT0-13B (a multilingual LLM) (Muen- nighoff et al., 2022), Google Translate (a widely used commercial service), GPT-4o (a SOTA LLM) (Hurst et al., 2024), and QWen2.5 instruction-tuned models (multilingual LLMs with 7B, 14B and 72B parameters) (Yang et al., 2024a;b). We sample 5 instances from each frequency range, resulting in 25 Chinese texts to be translated for each domain. In total, this produces 900 translation pairs from the nine systems. The authors, native speakers of both Chinese and English, manually translated each Chinese text into English. These translations are used as references in our analysis. 3.3 Human Annotation We recruit native Chinese speakers fluent in English through Prolific. The annotation process has two phases: the pilot phase and the main phase. In the pilot phase, 20 participants are provided with annotation guidelines and a quiz. The top 5 scorers are selected for the main phase. In the main phase, each translation pair is assigned to three annotators, ensuring equal workload distribution. The final annotations are determined by a majority vote, with a manual resolution by the first author for any ties. More details on inter-annotator agreement and quality control are provided in Appendix C.2. 4 Translation System Evaluation We conduct a detailed analysis of the idiom translation quality of the modern systems. We discuss how each system performs in each domain and their relative comparison. We then examine how domain and idiom frequency impact translation quality, followed by a discussion on the severity of translation errors. We present our results in Figure 3, which displays the ratio of different translation categories from each system in the four domains. Further analysis, such as commonly seen errors, subcategories selected, error distribution by frequency range, and average severity scores for each category, can be found in Appendix D. The following are the key findings: 1. GPT-4 and Qwen family have overall higher translation quality, while",
    "them once they are solved, we can more fully take advantage of the dif- ferent strengths of the LLM. When using the LLM to solve these sub-tasks, ideally we would know what the optimal prompt would be to solve the sub- task. As shown by the large body of work done in prompt engineering (Chen et al., 2025), LLMs are sensitive to small changes in prompt wording, and a portion of this can be ascribed to the inductive bias that is present in the LLM that gives it a \"pref- erence\" for certain wordings. Usually these issues 1 arXiv:2508.10295v1 [cs.CL] 14 Aug 2025 with prompt wording are solved through a repeti- tive prompt engineering process, but by prompting the LLM for its preference for prompt wording, we can eliminate the need for using prompt engi- neering to find the specific wording that works best with an LLM’s inductive biases. IBEaM is not in- herently limited to non-comparative classification tasks, but due to the difficulty of quantifying perfor- mance changes in other tasks, we limit the scope of this paper to this type of task. For each of the tasks that we examine in this paper, we prompt an LLM for one or more 10 point Likert scales that can be used to generate one or more scores for each instance for each task. These scores are combined to generate an overall score (or multiple overall scores if the task is multiple- choice) that is used to perform classification. For each task, we also evaluate a simple baseline where the LLM is prompted for a rating from 1 to 10 (or, again, multiple ratings if the task is multiple- choice) for each instance. Like the scores generated with our IBEaM method, these ratings are used to perform the downstream classification task. Our contributions are: • We define methods for inductive bias extrac- tion and including that extracted inductive bias in prompts to improve LLM performance. • We demonstrate that you can describe the cri- teria to evaluate and let the LLM identify its preferred wording. • We show that an LLM’s ability to generate numeric scores is limited, and we demonstrate that IBEaM can be used to generate improved numeric scores from LLMs. • We perform ablation studies that show that all components of IBEaM are useful. 2 Related Work With the advent of generative large language mod- els (LLMs) (Achiam et al., 2023; Touvron et al., 2023; Brown et al., 2020), much research has been conducted to determine what tasks they can be ap- plied to and what their efficacy is for those tasks (Zhao et al., 2025). While LLMs exhibit emergent behaviors that allow them to perform tasks that ma- chine learning methods were previously ineffective for, they can also be applied to tasks that existed prior to LLMs being introduced. The motivation for applying LLMs to these older tasks may be the desire to improve efficiency in execution speed, improve accessibility to non-technical users, or im- prove the state-of-the-art in accuracy. This broad spectrum of LLM applications is what has"
  ]
}