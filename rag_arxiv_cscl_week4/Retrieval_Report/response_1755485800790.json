{
  "query": "What is ReFT?",
  "results": [
    "Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts * Maxime Heuillet1,2, Yufei Cui3, Boxing Chen3, Audrey Durand1,2,4, Prasanna Parthasarathi3 1Universit´e Laval (IID) 2Mila - Qu´ebec AI Institute 3Huawei Noah’s Ark Lab (Montreal Research Center) 4Canada CIFAR AI Chair Abstract Advanced reasoning in LLMs on challenging domains like mathematical reasoning can be tackled using verifiable re- wards based reinforced fine-tuning (ReFT). In standard ReFT frameworks, a behavior model generates multiple comple- tions with answers per problem, for the answer to be then scored by a reward function. While such RL post-training methods demonstrate significant performance improvements across challenging reasoning domains, the computational cost of generating completions during training with multiple inference steps makes the training cost non-trivial. To address this, we draw inspiration from off-policy RL, and specula- tive decoding to introduce a novel ReFT framework, dubbed Nested-ReFT, where a subset of layers of the target model acts as the behavior model to generate off-policy comple- tions during training. The behavior model configured with dy- namic layer skipping per batch during training decreases the inference cost compared to the standard ReFT frameworks. Our theoretical analysis shows that Nested-ReFT yields un- biased gradient estimates with controlled variance. Our em- pirical analysis demonstrates improved computational effi- ciency measured as tokens/sec across multiple math rea- soning benchmarks and model sizes. Additionally, we ex- plore three variants of bias mitigation to minimize the off- policyness in the gradient updates that allows for maintaining performance that matches the baseline ReFT performance. Code — Under review. Introduction Large language models (LLMs) are increasingly capable at solving complex reasoning problems (Cobbe et al. 2021c). Such advances are enhanced by the LLMs ability to generate efficient chain-of-thoughts (CoT) through which the reason- ing is broken down textually into intermediate logical steps (Wei et al. 2022). Improving generalization performance on the CoT completions of LLMs using post-training tech- niques has gained popularity; which leverages the range of possible CoT solutions during training (Kumar et al. 2025; Shao et al. 2024; Xie et al. 2024; Silver et al. 2016). Modern LLM fine-tuning approaches leverage the range of possible CoT solutions by using reinforcement learning (RL) (Sutton et al. 2018). In RL-based fine-tuning, multiple * Manuscript submitted for review. CoT completions are rolled-out from a behavior model, and they are then scored using a reward function that is another model (Cobbe et al. 2021a; Lightman et al. 2023) or a simple heuristic (Luong et al. 2024). The scored completions are then used to propagate the gradients back to fine-tune the target model. The rollout process augments significantly the amount of available data to fine-tune the target LLM. RL-based fine-tuning with verifiable rewards corresponds to a specific fine-tuning problem called Reinforced Fine- tuning (ReFT) (Luong et al. 2024; Shao et al. 2024; Liu et al. 2025), which applies specifically to math and programming reasoning domains. As opposed to RL from human feedback (RLHF), a heuristic reward model can be used to verify and score the sampled completions instead of learning the re- ward model from human preferences (Rafailov et al. 2023).",
    "Retrace-λ Figure 2: Fine-tuning on GSM8k. Red annotations indicate the smallest value, and Green annotations the largest value. Model Instance SVAMP GSM8k Math12k ∆abs 1.5B Best +0.008 +0.015 +0.026 0.016 Worst −0.016 −0.006 −0.005 0.009 7B Best +0.022 +0.010 −0.003 0.009 Worst −0.070 −0.017 −0.022 0.036 Table 2: Best and worst observed deltas per dataset and model size. ∆abs is the mean absolute change to baseline. is smaller than that of the drops. In both cases, the worst and best performances imply at most ±2.6 points variation from the baseline performance. These results showcasing minor performance fluctuations corroborate the hypothesis that off- policy generations using Nested-ReFT have limited influ- ence on the performance on reasoning benchmarks. Impor- tantly, we highlight that some instances of Nested-ReFT yield performance improvements over the baseline while in- volving the generation of samples on a smaller model, indi- cating that nested models can deliver similar or better effect as full models. In this research, the strategy to generate off- policy samples is heuristic. The results suggest that special- ized learning strategies could improve further performance. Effectiveness of the off-policyness mitigation strategy We consider 3 off-polyciness mitigation strategies, namely Base, Practical and Retrace-λ. We observe that Retrace-λ displays the most stable performance across all models, fine-tuning datasets, and skipping ratios. Over the three datasets and two models (i.e. 6 configurations), the Base strategy achieves 1/6 best case count, 3/6 worst case count and 2/6 neutral count. The Practical strat- egy achieves 3/6 best case performance, 3/6 worst case, and 1/6 neutral count. This indicates that although Practical achieves peak performance, it is also unstable across config- urations. The Retrace-λ strategy achieves 2/6 best case, 0/6 worst case and 4/6 neutral. These results indicate that Retrace-λ offers overall more stable performance com- pared to the Base and Practical mitigation strategies. The results further support the potential of Retrace-λ (Munos et al. 2016) to mitigate off-policyness in the applica- tion of RL-based fine-tuning for LLMs. Other works (Roux et al. 2025) also point to Retrace-λ (Munos et al. 2016) to mitigate off-policyness in LLMs, but the setting covered in Roux et al. (2025) assumes a delayed and possibly fixed behavior model (e.g. a reference model, or a frozen earlier version). In contrast, we cover a different setting where off- policyness arises from a dynamic behavior model instanti- ated with a different architecture (nesting) than the target policy. 20 0 20 Accuracy (%) 0.126 0.121 0.152 1.5B model 0.192 0.170 7B model 10 0 10 20 Tok/sec (%) 4871.4 5514.0 3887.9 4424.7 0% 5% 10% 15% Skip Ratio 20 10 0 10 Runtime (%) 456 403 0% 5% 10% 15% Skip Ratio 572 503 Baseline Base Practical Retrace-λ Figure 3: Fine-tuning on Math12k. Red annotations indicate the smallest value, and Green annotations the largest value. Compute efficiency gains from off-policy roll-outs Following the theoretical analysis on the complexity, the ef- ficiency gains translate into linear trends on the total run- time, and on the token generation speed. This trend is ob- served in all the settings covered (see Figures 2, 3",
    "2024; Liu et al. 2025), which applies specifically to math and programming reasoning domains. As opposed to RL from human feedback (RLHF), a heuristic reward model can be used to verify and score the sampled completions instead of learning the re- ward model from human preferences (Rafailov et al. 2023). Despite appealing performance gains, RL-based fine- tuning has a higher computational, and memory cost com- pared to supervised fine-tuning (SFT) (Luong et al. 2024). While ReFT circumvent the memory cost of storing a re- ward model, the computational cost of sampling multiple completions from a behavior model can be overwhelming (Kazemnejad et al. 2025; Shao et al. 2024). This completion cost can add up significantly to the compute cost of updating the parameters of the target model. Practitioners currently sample 8 CoT completions per problem (von Werra et al. 2020), and the base setup consists in using as behavior model the same version of the target LLM (Luong et al. 2024; Shao et al. 2024). Although scal- ing up the number of CoT completions can lower the bias in the target model updates, it also adds a significant com- pute overhead. Therefore, there is a need to explore if the efficiency of the behavior model used for roll-outs can be improved. A broader impact would be to facilitate the gener- ation of more completions to further improve the reasoning performance of LLMs. Research question: Is it possible to improve the computa- tional efficiency of ReFT without compromising the perfor- mance of the fine-tuned target LLM? We hypothesize that: i) given a target LLM to fine-tune, it is possible to perform roll-outs from a behavior model that has a lower computa- tional cost than base formulations; ii) such low-cost roll-outs can be leveraged to update the target model with limited in- fluence on performance. arXiv:2508.10123v1 [cs.LG] 13 Aug 2025 Contributions We introduce a novel ReFT framework, dubbed Nested-ReFT, where a subset of layers of the target model acts as the behavior model to generate off- policy roll-outs during training. The nested behavior model decreases the inference cost compared to the standard ReFT frameworks. Our theoretical analysis shows that Nested-ReFT yields unbiased gradient estimates with controlled variance. Our empirical analysis demonstrates improved computational efficiency across multiple math reasoning benchmarks and model sizes. Additionally, we ex- plore three variants of bias mitigation to minimize the off- policyness in the gradient updates to maintain performance that matches the baseline ReFT performance. Problem definition Let X denote the space of possible prompts and Y denote the space of possible output sequences. Given a prompt xi ∈X, an LLM encodes a generating policy πθ, which defines a conditional probability distribution over output sequences ˆyi = (ˆyi,1, . . . , ˆyi,L) ∈Y, where L is the number of to- kens contained in the sequence. Let ˆyi,<ℓdenote the tokens (ˆyi,1, . . . , ˆyi,ℓ−1) in a sequence ˆyi. The probability of sam- pling sequence ˆyi given a prompt xi is defined in an auto- regressive manner: πθ(ˆyi|xi) = ΠL ℓ=1πθ(ˆyi,ℓ|xi, ˆyi,<ℓ), where πθ(ˆyi,ℓ|xi, ˆyi,<ℓ) is the probability of"
  ]
}